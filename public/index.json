
[{"content":" Fanzhe Meng # Role: Team Member\nGithub ","date":"11 February 2026","externalUrl":null,"permalink":"/team/fanzhe-meng/","section":"üöÄ Our Team","summary":"M.S. from RUC","title":"Fanzhe Meng","type":"team"},{"content":" Jie Chen # Role: Team Member\nGoogle Scholar ","date":"11 February 2026","externalUrl":null,"permalink":"/team/jie-chen/","section":"üöÄ Our Team","summary":"M.S. from RUC","title":"Jie Chen","type":"team"},{"content":" Jiale Zhao # Role: Team Member\nResearchGate ","date":"11 February 2026","externalUrl":null,"permalink":"/team/jiale-zhao/","section":"üöÄ Our Team","summary":"Ph.D. from ICT.","title":"Jiale Zhao","type":"team"},{"content":" Guoxin Chen # Role: Team Leader\nGoogle Scholar ","date":"11 February 2026","externalUrl":null,"permalink":"/team/guoxin-chen/","section":"üöÄ Our Team","summary":"Focusing on natural language processing and large language model research. Ph.D. from GSAI, RUC.","title":"Guoxin Chen","type":"team"},{"content":" Alice Zhang # Role: Team Lead \u0026amp; Senior Research Scientist\nFocus Areas: AI Agent Architectures, Large Language Models, Reinforcement Learning\nBio # Alice leads the Awe-AI Team with a focus on designing next-generation AI agent architectures. She received her Ph.D. in Computer Science from Tsinghua University, where she specialized in reinforcement learning and language model pre-training. Her research has been published at top venues including NeurIPS, ICML, and ACL.\nSelected Publications # \u0026ldquo;Autonomous Software Engineering with Multi-Agent Systems\u0026rdquo; ‚Äî NeurIPS 2025 \u0026ldquo;Scaling Laws for Code Generation Models\u0026rdquo; ‚Äî ICML 2025 \u0026ldquo;Context-Aware Planning in LLM Agents\u0026rdquo; ‚Äî ACL 2025 Contact # Email: alice.zhang@example.com Google Scholar | GitHub ","date":"1 January 2026","externalUrl":null,"permalink":"/team/alice-zhang/","section":"üöÄ Our Team","summary":"Team Lead focusing on AI agent architectures and large language model research. Ph.D. from Tsinghua University.","title":"Alice Zhang","type":"team"},{"content":" Bob Li # Role: Senior Engineer ‚Äî Infrastructure \u0026amp; Systems\nFocus Areas: Distributed Systems, Container Orchestration, Platform Engineering\nBio # Bob is the infrastructure backbone of Awe-AI. He designed and built the SWALM Portal service and the container-based execution environment that powers all our agent evaluations. With 8+ years of experience in distributed systems, he ensures our AI agents have reliable, scalable, and secure execution environments.\nKey Contributions # Designed the Portal architecture for in-container operations Built the Environment Manager for automated container lifecycle management Developed the evaluation proxy service for scalable benchmarking Optimized container startup times by 60% through layered caching Contact # Email: bob.li@example.com GitHub ","date":"1 January 2026","externalUrl":null,"permalink":"/team/bob-li/","section":"üöÄ Our Team","summary":"Senior Engineer specializing in distributed systems and AI infrastructure. Architect of the SWALM Portal and container orchestration platform.","title":"Bob Li","type":"team"},{"content":" Carol Wang # Role: Research Scientist ‚Äî Multimodal AI\nFocus Areas: Vision-Language Models, Document Understanding, Visual Reasoning\nBio # Carol is a research scientist specializing in multimodal AI systems. Her work bridges computer vision and natural language processing, enabling AI agents to understand and reason over visual information. She received her Ph.D. from Peking University and has published extensively at CVPR, ECCV, and AAAI.\nSelected Publications # \u0026ldquo;Cross-Modal Reasoning for Software UI Understanding\u0026rdquo; ‚Äî CVPR 2025 \u0026ldquo;Document-Aware Code Generation from Screenshots\u0026rdquo; ‚Äî ECCV 2024 \u0026ldquo;Visual Chain-of-Thought Prompting\u0026rdquo; ‚Äî AAAI 2025 Contact # Email: carol.wang@example.com Google Scholar | GitHub ","date":"1 January 2026","externalUrl":null,"permalink":"/team/carol-wang/","section":"üöÄ Our Team","summary":"Research Scientist working on multimodal reasoning and vision-language models. Published at CVPR, ECCV, and AAAI.","title":"Carol Wang","type":"team"},{"content":" David Chen # Role: Research Engineer ‚Äî Code Intelligence\nFocus Areas: Program Synthesis, Code Understanding, Developer Tools\nBio # David bridges the gap between research and product by building practical AI-powered developer tools. He has extensive experience with code representation learning, static analysis, and IDE integration. Before joining Awe-AI, he contributed to several open-source developer tools.\nKey Contributions # Developed the CodeAct agent implementation for automated coding tasks Built code search and retrieval pipelines for repository-level understanding Created developer tool integrations for VS Code and JetBrains IDEs Contributed to the SWE-Bench evaluation framework Contact # Email: david.chen@example.com GitHub ","date":"1 January 2026","externalUrl":null,"permalink":"/team/david-chen/","section":"üöÄ Our Team","summary":"Research Engineer focused on code intelligence, program synthesis, and AI-assisted software development tools.","title":"David Chen","type":"team"},{"content":" Emma Liu # Role: Research Scientist ‚Äî Data \u0026amp; Training\nFocus Areas: Data Curation, Model Fine-Tuning, RLHF, Alignment\nBio # Emma leads our data and training pipeline efforts. She designs the data collection, filtering, and curation strategies that produce high-quality training data for our code-specialized models. Her expertise in RLHF and alignment techniques ensures our models are both capable and reliable.\nKey Contributions # Designed the SWE Data Engine pipeline for automated training data generation Developed quality filtering algorithms that improved training data utility by 40% Implemented RLHF training loops for code agent alignment Created preference data generation from agent trajectories Contact # Email: emma.liu@example.com Google Scholar | GitHub ","date":"1 January 2026","externalUrl":null,"permalink":"/team/emma-liu/","section":"üöÄ Our Team","summary":"Research Scientist specializing in training data curation, model fine-tuning, and alignment techniques for code LLMs.","title":"Emma Liu","type":"team"},{"content":" Frank Wu # Role: Engineer ‚Äî Evaluation \u0026amp; Benchmarking\nFocus Areas: Benchmark Design, Evaluation Infrastructure, Performance Analysis\nBio # Frank is responsible for building and maintaining our evaluation infrastructure. He designs benchmarks that meaningfully measure agent capabilities and builds the tooling needed to run evaluations at scale. His work ensures we have reliable, reproducible measurements of agent performance.\nKey Contributions # Built the Awe-Bench evaluation suite with 500+ real-world tasks Designed the Terminal-Bench benchmark for command-line agent evaluation Created automated regression testing for agent releases Developed performance dashboards and analytics pipelines Contact # Email: frank.wu@example.com GitHub ","date":"1 January 2026","externalUrl":null,"permalink":"/team/frank-wu/","section":"üöÄ Our Team","summary":"Engineer specializing in evaluation systems, benchmark design, and performance analysis for AI agents.","title":"Frank Wu","type":"team"},{"content":"We are a diverse group composed of Ph.D. and M.S. students from RUC, currently interning at ByteDance\u0026rsquo;s BandAI department, we aim to build smarter AI agents highly professional in the fields of reasoning, deep search, and code.\nüåü High Lights # Intelligent General Agents: Developing agents capable of complex reasoning, deep search, and coding. Open Collaboration: Connecting academic research (RUC) with industrial practice (ByteDance). ","date":"11 February 2026","externalUrl":null,"permalink":"/team/","section":"üöÄ Our Team","summary":"We are a diverse group composed of Ph.D. and M.S. students from RUC, currently interning at ByteDance‚Äôs BandAI department, we aim to build smarter AI agents highly professional in the fields of reasoning, deep search, and code.\nüåü High Lights # Intelligent General Agents: Developing agents capable of complex reasoning, deep search, and coding. Open Collaboration: Connecting academic research (RUC) with industrial practice (ByteDance). ","title":"üöÄ Our Team","type":"team"},{"content":"","date":"11 February 2026","externalUrl":null,"permalink":"/tags/agent/","section":"Tags","summary":"","title":"Agent","type":"tags"},{"content":"","date":"11 February 2026","externalUrl":null,"permalink":"/tags/ai4science/","section":"Tags","summary":"","title":"AI4Science","type":"tags"},{"content":"We are dedicated to developing advanced General AI Agents and specialized Vertical Domain solutions, aiming to push the boundaries of AGI by constructing realistic and safe environments, and also building, evaluating agents in complex, real-world scenarios. Explore our latest projects or meet our team members.\n","date":"11 February 2026","externalUrl":null,"permalink":"/","section":"Awe-AI","summary":"We are dedicated to developing advanced General AI Agents and specialized Vertical Domain solutions, aiming to push the boundaries of AGI by constructing realistic and safe environments, and also building, evaluating agents in complex, real-world scenarios. Explore our latest projects or meet our team members.\n","title":"Awe-AI","type":"page"},{"content":"","date":"11 February 2026","externalUrl":null,"permalink":"/tags/crs/","section":"Tags","summary":"","title":"CRS","type":"tags"},{"content":"","date":"11 February 2026","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"","date":"11 February 2026","externalUrl":null,"permalink":"/tags/nlp/","section":"Tags","summary":"","title":"NLP","type":"tags"},{"content":"","date":"11 February 2026","externalUrl":null,"permalink":"/tags/pretrain/","section":"Tags","summary":"","title":"Pretrain","type":"tags"},{"content":"","date":"11 February 2026","externalUrl":null,"permalink":"/tags/research/","section":"Tags","summary":"","title":"Research","type":"tags"},{"content":"","date":"11 February 2026","externalUrl":null,"permalink":"/tags/rl/","section":"Tags","summary":"","title":"RL","type":"tags"},{"content":"","date":"11 February 2026","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"10 February 2026","externalUrl":null,"permalink":"/categories/artificial-intelligence/","section":"Categories","summary":"","title":"Artificial Intelligence","type":"categories"},{"content":"","date":"10 February 2026","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"10 February 2026","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"10 February 2026","externalUrl":null,"permalink":"/tags/code-agent/","section":"Tags","summary":"","title":"Code Agent","type":"tags"},{"content":" Overview # Achieving mastery in real world software engineering tasks is fundamentally bottlenecked by the scarcity of large scale, high quality training data. Scaling such data has been limited by the complexity of environment setup, unit test generation, and problem statement curation. In this paper, we propose ScaleSWE, an automated, sandboxed multi agent workflow designed to construct high quality SWE data at scale. The system coordinates three specialized agents for environment setup, test creation, and problem description synthesis to process 6 million pull requests across 5200 repositories, producing Scale SWE Data: 100k verified SWE instances, the largest such dataset to date. It substantially surpasses existing real world datasets in repository diversity and reflects realistic task complexity. We further demonstrate the dataset utility for training by distilling 71498 high quality trajectories and finetuning Qwen30BA3BInstruct to produce ScaleSWE Agent. Our agent achieves a 64 resolve rate on SWE Bench Verified a nearly three fold improvement over the base model. ScaleSWE provides a scalable, reproducible approach for data construction to advance LLM based software engineering. Scale SWE will be publicly available.\nPerformance # Links # Arxiv Dataset on HuggingFaceHuggingFace Model on HuggingFaceHuggingFace ","date":"10 February 2026","externalUrl":null,"permalink":"/projects/scaleswe/","section":"Projects \u0026 Research","summary":"Overview # Achieving mastery in real world software engineering tasks is fundamentally bottlenecked by the scarcity of large scale, high quality training data. Scaling such data has been limited by the complexity of environment setup, unit test generation, and problem statement curation. In this paper, we propose ScaleSWE, an automated, sandboxed multi agent workflow designed to construct high quality SWE data at scale. The system coordinates three specialized agents for environment setup, test creation, and problem description synthesis to process 6 million pull requests across 5200 repositories, producing Scale SWE Data: 100k verified SWE instances, the largest such dataset to date. It substantially surpasses existing real world datasets in repository diversity and reflects realistic task complexity. We further demonstrate the dataset utility for training by distilling 71498 high quality trajectories and finetuning Qwen30BA3BInstruct to produce ScaleSWE Agent. Our agent achieves a 64 resolve rate on SWE Bench Verified a nearly three fold improvement over the base model. ScaleSWE provides a scalable, reproducible approach for data construction to advance LLM based software engineering. Scale SWE will be publicly available.\n","title":"Immersion in the GitHub Universe: Scaling Coding Agents to Mastery","type":"projects"},{"content":"","date":"10 February 2026","externalUrl":null,"permalink":"/authors/jiale-zhao-guoxin-chen-fanzhe-meng-minghao-li-jie-chen-hui-xu-yongshuai-sun-xin-zhao-ruihua-song-yuan-zhang-peng-wang-cheng-chen-jirong-wen-kai-jia/","section":"Authors","summary":"","title":"Jiale Zhao*, Guoxin Chen*, Fanzhe Meng*, Minghao Li, Jie Chen, Hui Xu, Yongshuai Sun, Xin Zhao, Ruihua Song, Yuan Zhang, Peng Wang, Cheng Chen, Jirong Wen, Kai Jia","type":"authors"},{"content":"Discover the latest advancements from the Awe-AI Team. We are committed to open research and building tools that push the boundaries of what AI can achieve.\n","date":"10 February 2026","externalUrl":null,"permalink":"/projects/","section":"Projects \u0026 Research","summary":"Discover the latest advancements from the Awe-AI Team. We are committed to open research and building tools that push the boundaries of what AI can achieve.\n","title":"Projects \u0026 Research","type":"projects"},{"content":"","date":"10 February 2026","externalUrl":null,"permalink":"/tags/software-engineering/","section":"Tags","summary":"","title":"Software Engineering","type":"tags"},{"content":"","date":"31 January 2026","externalUrl":null,"permalink":"/tags/agentic-rl/","section":"Tags","summary":"","title":"Agentic RL","type":"tags"},{"content":"","date":"31 January 2026","externalUrl":null,"permalink":"/tags/deep-research/","section":"Tags","summary":"","title":"Deep Research","type":"tags"},{"content":"","date":"31 January 2026","externalUrl":null,"permalink":"/authors/guoxin-chen-zile-qiao-xuanzhong-chen-donglei-yu-haotian-xu-wayne-xin-zhao-ruihua-song-wenbiao-yin-huifeng-yin-liwen-zhang-kuan-li-minpeng-liao-yong-jiang-pengjun-xie-fei-huang-jingren-zhou/","section":"Authors","summary":"","title":"Guoxin Chen, Zile Qiao, Xuanzhong Chen, Donglei Yu, Haotian Xu, Wayne Xin Zhao, Ruihua Song, Wenbiao Yin, Huifeng Yin, Liwen Zhang, Kuan Li, Minpeng Liao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou","type":"authors"},{"content":" Overview # Recent advances in deep-research agents have shown promise for autonomous knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates all information in a single, expanding context window, leading to context suffocation and noise contamination that limit their effectiveness on long-horizon tasks. We introduce IterResearch, a novel iterative deep-research paradigm that revisits long-horizon research through the lens of Interaction Scaling. Instead of relying on linear context accumulation, we adopt an MDP-inspired architecture with strategic workspace reconstruction. By maintaining an evolving report as memory and periodically synthesizing insights, our approach preserves consistent reasoning capacity across arbitrary exploration depths. To effectively train this paradigm, we employ Efficiency-Aware Policy Optimization (EAPO), a training strategy that adapts geometric reward discounting to incentivize efficient exploration and utilizes adaptive downsampling for stable distributed training. Extensive experiments demonstrate that IterResearch achieves substantial improvements over existing open-source agents with average +14.5pp across six benchmarks and narrows the gap with frontier proprietary systems. Remarkably, our paradigm exhibits unprecedented interaction scaling, extending to 2048 interactions with dramatic performance gains (from 3.5% to 42.5%), and serves as an effective prompting strategy, improving frontier models by up to 19.2pp over ReAct on long-horizon tasks. These findings position IterResearch as a versatile solution for long-horizon reasoning, effective both as a trained agent and as a prompting paradigm for frontier models.\nKey Features # We propose IterResearch, a novel iterative deep-research paradigm that revisits long-horizon research through an MDP-based formalism, maintaining sustained reasoning capacity through periodic synthesis and an evolving report memory‚Äîeliminating the context suffocation and noise contamination that plague mono-contextual approaches. We employ Efficiency-Aware Policy Optimization (EAPO) which adapts geometric discounted rewards that incentivize efficient exploration and utilizes adaptive downsampling for stable distributed training, enabling effective learning from our paradigm‚Äôs unique trajectory structure. We demonstrate IterResearch‚Äôs exceptional capabilities and broader impact: (1) achieving an average 14.5 pp improvement across six challenging benchmarks; (2) exhibiting interaction scaling to 2048 interactions with dramatic performance gains; (3) enabling cross-paradigm knowledge transfer to enhance mono-contextual agents; (4) providing a model-agnostic prompting strategy that significantly improves frontier models on long-horizon tasks without training. Overall Performance # Main Results # Ablation Studies # Performance comparison between IterResearch and ReAct as Prompting Strategies # Links # Dataset on HuggingFace Pipeline Documentation ","date":"31 January 2026","externalUrl":null,"permalink":"/projects/iter-research/","section":"Projects \u0026 Research","summary":"Overview # Recent advances in deep-research agents have shown promise for autonomous knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates all information in a single, expanding context window, leading to context suffocation and noise contamination that limit their effectiveness on long-horizon tasks. We introduce IterResearch, a novel iterative deep-research paradigm that revisits long-horizon research through the lens of Interaction Scaling. Instead of relying on linear context accumulation, we adopt an MDP-inspired architecture with strategic workspace reconstruction. By maintaining an evolving report as memory and periodically synthesizing insights, our approach preserves consistent reasoning capacity across arbitrary exploration depths. To effectively train this paradigm, we employ Efficiency-Aware Policy Optimization (EAPO), a training strategy that adapts geometric reward discounting to incentivize efficient exploration and utilizes adaptive downsampling for stable distributed training. Extensive experiments demonstrate that IterResearch achieves substantial improvements over existing open-source agents with average +14.5pp across six benchmarks and narrows the gap with frontier proprietary systems. Remarkably, our paradigm exhibits unprecedented interaction scaling, extending to 2048 interactions with dramatic performance gains (from 3.5% to 42.5%), and serves as an effective prompting strategy, improving frontier models by up to 19.2pp over ReAct on long-horizon tasks. These findings position IterResearch as a versatile solution for long-horizon reasoning, effective both as a trained agent and as a prompting paradigm for frontier models.\n","title":"IterResearch: Rethinking Long-Horizon Agents with Interaction Scaling","type":"projects"},{"content":"","date":"20 January 2026","externalUrl":null,"permalink":"/authors/awe-ai-team/","section":"Authors","summary":"","title":"Awe-AI Team","type":"authors"},{"content":" Overview # Awe-Bench is our comprehensive evaluation benchmark suite that rigorously tests AI agents on real-world software engineering challenges. Unlike traditional benchmarks, Awe-Bench incorporates diverse task types, realistic containerized development environments, and fine-grained evaluation metrics.\nKey Features # Real-World Tasks: Curated from actual open-source repository issues and pull requests Diverse Task Types: Bug fixing, feature implementation, code refactoring, documentation, and testing Containerized Evaluation: Each task runs in an isolated Docker environment with pre-configured dependencies Multi-Dimensional Metrics: Beyond pass/fail ‚Äî measures code quality, test coverage, and solution efficiency Scalable Infrastructure: Supports parallel evaluation across hundreds of tasks Benchmark Categories # SWE Tasks # Traditional software engineering tasks derived from popular open-source projects, including Django, Flask, scikit-learn, and more.\nTerminal Tasks # Command-line oriented challenges that test an agent\u0026rsquo;s ability to navigate file systems, manage processes, and use development tools effectively.\nMath Reasoning # Mathematical problem-solving tasks that require formal reasoning and proof construction capabilities.\nGetting Started # # Run a single SWE task evaluation uv run packages/swalm-core/src/swalm/task/swe/examples/run_single.py # Run the full benchmark suite python -m swalm.eval --config configs/eval/awe_bench.yaml Links # Benchmark Leaderboard Dataset on HuggingFace ","date":"20 January 2026","externalUrl":null,"permalink":"/projects/awe-bench/","section":"Projects \u0026 Research","summary":"Awe-Bench is our evaluation benchmark suite designed to rigorously test AI agents on real-world software engineering challenges. It goes beyond traditional benchmarks by incorporating diverse task types, realistic development environments, and fine-grained evaluation metrics.","title":"Awe-Bench: Next-Generation Evaluation for AI Agents","type":"projects"},{"content":"","date":"20 January 2026","externalUrl":null,"permalink":"/tags/benchmark/","section":"Tags","summary":"","title":"Benchmark","type":"tags"},{"content":"","date":"20 January 2026","externalUrl":null,"permalink":"/tags/evaluation/","section":"Tags","summary":"","title":"Evaluation","type":"tags"},{"content":"","date":"20 January 2026","externalUrl":null,"permalink":"/tags/open-source/","section":"Tags","summary":"","title":"Open-Source","type":"tags"},{"content":"","date":"20 January 2026","externalUrl":null,"permalink":"/categories/research/","section":"Categories","summary":"","title":"Research","type":"categories"},{"content":"","date":"10 January 2026","externalUrl":null,"permalink":"/tags/multimodal/","section":"Tags","summary":"","title":"Multimodal","type":"tags"},{"content":" Overview # Our multimodal reasoning research focuses on building AI systems that seamlessly integrate visual perception with language understanding. We explore novel architectures for cross-modal reasoning, enabling AI agents to understand documents, diagrams, user interfaces, and real-world scenes.\nResearch Directions # Document Understanding # Training models to parse and reason over complex documents including tables, charts, and mixed-format layouts.\nUI Comprehension # Enabling AI agents to understand graphical user interfaces, supporting automated testing and interaction.\nVisual Reasoning # Developing models that can perform step-by-step reasoning over visual inputs, combining perception with logical deduction.\nKey Results # State-of-the-art performance on document QA benchmarks Novel cross-attention mechanisms for vision-language alignment Efficient training strategies for multimodal foundation models Applications # Our multimodal reasoning capabilities power several downstream applications:\nAutomated Code Review: Understanding code changes alongside visual diffs Bug Report Triage: Parsing screenshots and error messages together Documentation Generation: Creating descriptions from UI screenshots Links # Research Paper (ArXiv) Model Weights (HuggingFace) ","date":"10 January 2026","externalUrl":null,"permalink":"/projects/multimodal-reasoning/","section":"Projects \u0026 Research","summary":"Our multimodal reasoning research focuses on building AI systems that seamlessly integrate visual perception with language understanding. We explore novel architectures for cross-modal reasoning, enabling agents to understand documents, diagrams, UIs, and real-world scenes.","title":"Multimodal Reasoning: Bridging Vision and Language","type":"projects"},{"content":"","date":"10 January 2026","externalUrl":null,"permalink":"/tags/vision-language/","section":"Tags","summary":"","title":"Vision-Language","type":"tags"},{"content":"","date":"1 January 2026","externalUrl":null,"permalink":"/tags/code-intelligence/","section":"Tags","summary":"","title":"Code Intelligence","type":"tags"},{"content":"","date":"1 January 2026","externalUrl":null,"permalink":"/tags/data/","section":"Tags","summary":"","title":"Data","type":"tags"},{"content":"","date":"1 January 2026","externalUrl":null,"permalink":"/tags/engineering/","section":"Tags","summary":"","title":"Engineering","type":"tags"},{"content":"","date":"1 January 2026","externalUrl":null,"permalink":"/tags/infrastructure/","section":"Tags","summary":"","title":"Infrastructure","type":"tags"},{"content":"","date":"1 January 2026","externalUrl":null,"permalink":"/tags/leadership/","section":"Tags","summary":"","title":"Leadership","type":"tags"},{"content":"","date":"1 January 2026","externalUrl":null,"permalink":"/tags/training/","section":"Tags","summary":"","title":"Training","type":"tags"},{"content":"","date":"15 December 2025","externalUrl":null,"permalink":"/tags/code-generation/","section":"Tags","summary":"","title":"Code Generation","type":"tags"},{"content":" Overview # Our code generation research explores cutting-edge techniques for translating natural language intent into production-quality code. We develop specialized models and training pipelines that understand programming semantics, project context, and software engineering best practices.\nKey Innovations # Context-Aware Generation # Our models leverage repository-level context ‚Äî including file dependencies, coding conventions, and API usage patterns ‚Äî to generate code that fits naturally within existing codebases.\nMulti-Step Planning # Rather than generating code in a single pass, our approach decomposes complex requirements into a sequence of planned steps, improving both accuracy and maintainability.\nSelf-Repair Mechanisms # Built-in iterative refinement allows models to detect and fix errors in their own generated code through test execution and static analysis feedback loops.\nSupported Languages # Python, JavaScript/TypeScript, Java, C++, Go, Rust SQL, Bash/Shell scripting Infrastructure-as-Code (Terraform, Docker, Kubernetes) Benchmarks # Benchmark Score Rank HumanEval 92.1% Top 3 MBPP 88.5% Top 5 SWE-Bench Lite 45.2% Top 5 Links # Model on HuggingFace Technical Report ","date":"15 December 2025","externalUrl":null,"permalink":"/projects/code-generation/","section":"Projects \u0026 Research","summary":"Our code generation research explores cutting-edge techniques for translating natural language intent into production-quality code. We develop specialized models and training pipelines that understand programming semantics, project context, and software engineering best practices.","title":"Code Generation: From Intent to Implementation","type":"projects"},{"content":" Overview # Our Agent Framework provides the foundational building blocks for creating autonomous AI systems. It offers a modular architecture with pluggable components for reasoning, tool use, memory management, and environment interaction.\nArchitecture # The framework follows a clean, layered design:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Agent Layer ‚îÇ ‚îÇ (CodeAct, SWE, Cline, etc.) ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Tool Layer ‚îÇ ‚îÇ (Shell, Editor, Browser, etc.) ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Client Layer ‚îÇ ‚îÇ (Portal, EnvManager, LLM) ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Infrastructure Layer ‚îÇ ‚îÇ (Container, Runtime, Logging) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Key Components # Agent Base Class # A flexible base class that handles the core agent loop: observe ‚Üí think ‚Üí act ‚Üí evaluate.\nTool Registry # A dynamic tool registration system supporting both stateless tools (search, browsing) and stateful tools (file editing, shell execution).\nMemory Management # Configurable memory strategies including sliding window, summarization, and retrieval-augmented context management.\nEnvironment Abstraction # Unified interface for local execution, container-based sandboxing, and remote environment management.\nQuick Start # from swalm.core.agent.base import AgentBase class MyAgent(AgentBase): def __init__(self, llm_config, portal_config): super().__init__(llm_config, portal_config) self.tools = [...] async def run(self, prompt: str, max_turn: int = 30): # Your agent logic here pass Links # GitHub Repository API Documentation ","date":"28 November 2025","externalUrl":null,"permalink":"/projects/agent-framework/","section":"Projects \u0026 Research","summary":"Our Agent Framework provides the foundational building blocks for creating autonomous AI systems. It offers a modular architecture with pluggable components for reasoning, tool use, memory management, and environment interaction ‚Äî enabling rapid prototyping of novel agent designs.","title":"Agent Framework: Building Blocks for Autonomous AI","type":"projects"},{"content":"","date":"28 November 2025","externalUrl":null,"permalink":"/tags/framework/","section":"Tags","summary":"","title":"Framework","type":"tags"},{"content":"","date":"28 November 2025","externalUrl":null,"permalink":"/categories/open-source/","section":"Categories","summary":"","title":"Open-Source","type":"categories"},{"content":" Overview # The SWE Data Engine is our automated pipeline for generating high-quality training data from real-world software repositories. It transforms raw repository history into structured training examples at scale.\nPipeline Stages # 1. Repository Mining # Automated collection and filtering of repositories based on quality metrics, activity levels, and language distribution.\n2. Issue-PR Linking # Intelligent matching of issues to their corresponding pull requests, creating natural (problem, solution) pairs.\n3. Environment Reproduction # Automated Docker environment generation that faithfully reproduces the build and test setup at each commit point.\n4. Quality Filtering # Multi-stage filtering pipeline that ensures data quality through:\nTest validation (solutions must pass existing tests) Diff complexity analysis Code quality scoring Deduplication 5. Data Formatting # Conversion into various training formats:\nSFT (Supervised Fine-Tuning) trajectories RLHF preference pairs Agent interaction traces Scale # Metric Count Repositories Processed 50,000+ Training Examples 500,000+ Languages Covered 15+ Docker Environments 10,000+ Links # Dataset on HuggingFace Pipeline Documentation ","date":"10 November 2025","externalUrl":null,"permalink":"/projects/data-engine/","section":"Projects \u0026 Research","summary":"The SWE Data Engine is our automated pipeline for generating high-quality training data from real-world software repositories. It transforms raw repository history into structured training examples ‚Äî including bug fixes, feature implementations, and code reviews ‚Äî at scale.","title":"SWE Data Engine: Scalable Training Data for Code Agents","type":"projects"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]