<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects &amp; Research on Awe-AI</title>
    <link>http://localhost:1313/projects/</link>
    <description>Recent content in Projects &amp; Research on Awe-AI</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>© 2026 Awe-AI Team. All rights reserved.</copyright>
    <lastBuildDate>Tue, 10 Feb 2026 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/projects/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Immersion in the GitHub Universe: Scaling Coding Agents to Mastery</title>
      <link>http://localhost:1313/projects/scaleswe/</link>
      <pubDate>Tue, 10 Feb 2026 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/projects/scaleswe/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Link:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2602.09892&#34;  target=&#34;_blank&#34; rel=&#34;noreferrer&#34;&gt;https://arxiv.org/abs/2602.09892&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Repository Link:&lt;/strong&gt; &lt;a href=&#34;https://github.com/AweAI-Team/ScaleSWE&#34;  target=&#34;_blank&#34; rel=&#34;noreferrer&#34;&gt;https://github.com/AweAI-Team/ScaleSWE&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Data Link:&lt;/strong&gt; &lt;a href=&#34;https://huggingface.co/datasets/Awe-AI/Scale-SWE&#34;  target=&#34;_blank&#34; rel=&#34;noreferrer&#34;&gt;https://huggingface.co/datasets/Awe-AI/Scale-SWE&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;We propose the &lt;strong&gt;Scale-SWE&lt;/strong&gt; dataset. By utilizing a Sandboxed multi-agent system, we achieved the scaling of SWE tasks to construct 100k real SWE data points, creating the &lt;strong&gt;largest open-source high-quality SWE dataset&lt;/strong&gt; to date. This makes it possible to fully train Code Agents on GitHub-scale data.&lt;/p&gt;&#xA;&lt;p&gt;Furthermore, by utilizing distilled data to train &lt;strong&gt;Qwen3-30A3B-Instruct&lt;/strong&gt;, we achieved a score of &lt;strong&gt;64% on SWE-bench-Verified&lt;/strong&gt;. This makes it possible for academic models of the same size to &lt;strong&gt;surpass industrial models&lt;/strong&gt; (such as GLM-4.7-Flash)!&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/projects/scaleswe/feature.png" />
    </item>
    
    <item>
      <title>IterResearch: Rethinking Long-Horizon Agents with Interaction Scaling</title>
      <link>http://localhost:1313/projects/iter-research/</link>
      <pubDate>Sat, 31 Jan 2026 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/projects/iter-research/</guid>
      <description>&lt;h2 class=&#34;relative group&#34;&gt;Overview&#xA;    &lt;div id=&#34;overview&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;&#xA;    &#xA;    &lt;span&#xA;        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none&#34;&gt;&#xA;        &lt;a class=&#34;text-primary-300 dark:text-neutral-700 !no-underline&#34; href=&#34;#overview&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;&#xA;    &lt;/span&gt;&#xA;    &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Recent advances in deep-research agents have shown promise for autonomous&#xA;knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates&#xA;all information in a single, expanding context window, leading to context suffocation and noise contamination that limit their effectiveness on long-horizon&#xA;tasks. We introduce IterResearch, a novel iterative deep-research paradigm that&#xA;revisits long-horizon research through the lens of Interaction Scaling. Instead of&#xA;relying on linear context accumulation, we adopt an MDP-inspired architecture&#xA;with strategic workspace reconstruction. By maintaining an evolving report as&#xA;memory and periodically synthesizing insights, our approach preserves consistent reasoning capacity across arbitrary exploration depths. To effectively train&#xA;this paradigm, we employ Efficiency-Aware Policy Optimization (EAPO), a training strategy that adapts geometric reward discounting to incentivize efficient exploration and utilizes adaptive downsampling for stable distributed training. Extensive experiments demonstrate that IterResearch achieves substantial improvements over existing open-source agents with average +14.5pp across six benchmarks and narrows the gap with frontier proprietary systems. Remarkably, our&#xA;paradigm exhibits unprecedented interaction scaling, extending to 2048 interactions with dramatic performance gains (from 3.5% to 42.5%), and serves as an effective prompting strategy, improving frontier models by up to 19.2pp over ReAct&#xA;on long-horizon tasks. These findings position IterResearch as a versatile solution&#xA;for long-horizon reasoning, effective both as a trained agent and as a prompting&#xA;paradigm for frontier models.&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/projects/iter-research/feature.png" />
    </item>
    
    <item>
      <title>Awe-Bench: Next-Generation Evaluation for AI Agents</title>
      <link>http://localhost:1313/projects/awe-bench/</link>
      <pubDate>Tue, 20 Jan 2026 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/projects/awe-bench/</guid>
      <description>Awe-Bench is our evaluation benchmark suite designed to rigorously test AI agents on real-world software engineering challenges. It goes beyond traditional benchmarks by incorporating diverse task types, realistic development environments, and fine-grained evaluation metrics.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/projects/awe-bench/feature.svg" />
    </item>
    
    <item>
      <title>Multimodal Reasoning: Bridging Vision and Language</title>
      <link>http://localhost:1313/projects/multimodal-reasoning/</link>
      <pubDate>Sat, 10 Jan 2026 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/projects/multimodal-reasoning/</guid>
      <description>Our multimodal reasoning research focuses on building AI systems that seamlessly integrate visual perception with language understanding. We explore novel architectures for cross-modal reasoning, enabling agents to understand documents, diagrams, UIs, and real-world scenes.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/projects/multimodal-reasoning/feature.svg" />
    </item>
    
    <item>
      <title>Code Generation: From Intent to Implementation</title>
      <link>http://localhost:1313/projects/code-generation/</link>
      <pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/projects/code-generation/</guid>
      <description>Our code generation research explores cutting-edge techniques for translating natural language intent into production-quality code. We develop specialized models and training pipelines that understand programming semantics, project context, and software engineering best practices.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/projects/code-generation/feature.svg" />
    </item>
    
    <item>
      <title>Agent Framework: Building Blocks for Autonomous AI</title>
      <link>http://localhost:1313/projects/agent-framework/</link>
      <pubDate>Fri, 28 Nov 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/projects/agent-framework/</guid>
      <description>Our Agent Framework provides the foundational building blocks for creating autonomous AI systems. It offers a modular architecture with pluggable components for reasoning, tool use, memory management, and environment interaction — enabling rapid prototyping of novel agent designs.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/projects/agent-framework/feature.svg" />
    </item>
    
    <item>
      <title>SWE Data Engine: Scalable Training Data for Code Agents</title>
      <link>http://localhost:1313/projects/data-engine/</link>
      <pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/projects/data-engine/</guid>
      <description>The SWE Data Engine is our automated pipeline for generating high-quality training data from real-world software repositories. It transforms raw repository history into structured training examples — including bug fixes, feature implementations, and code reviews — at scale.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/projects/data-engine/feature.svg" />
    </item>
    
  </channel>
</rss>
