<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Projects &amp; Research on Awe-AI</title><link>//localhost:1313/projects/</link><description>Recent content in Projects &amp; Research on Awe-AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© {year} Awe-AI Team. All rights reserved.</copyright><lastBuildDate>Sat, 31 Jan 2026 00:00:00 +0000</lastBuildDate><atom:link href="//localhost:1313/projects/index.xml" rel="self" type="application/rss+xml"/><item><title>IterResearch: Rethinking Long-Horizon Agents with Interaction Scaling</title><link>//localhost:1313/projects/iter-research/</link><pubDate>Sat, 31 Jan 2026 00:00:00 +0000</pubDate><guid>//localhost:1313/projects/iter-research/</guid><description>&lt;h2 class="relative group"&gt;Overview
 &lt;div id="overview" class="anchor"&gt;&lt;/div&gt;
 
 &lt;span
 class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"&gt;
 &lt;a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#overview" aria-label="Anchor"&gt;#&lt;/a&gt;
 &lt;/span&gt;
 
&lt;/h2&gt;
&lt;p&gt;Recent advances in deep-research agents have shown promise for autonomous
knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates
all information in a single, expanding context window, leading to context suffocation and noise contamination that limit their effectiveness on long-horizon
tasks. We introduce IterResearch, a novel iterative deep-research paradigm that
revisits long-horizon research through the lens of Interaction Scaling. Instead of
relying on linear context accumulation, we adopt an MDP-inspired architecture
with strategic workspace reconstruction. By maintaining an evolving report as
memory and periodically synthesizing insights, our approach preserves consistent reasoning capacity across arbitrary exploration depths. To effectively train
this paradigm, we employ Efficiency-Aware Policy Optimization (EAPO), a training strategy that adapts geometric reward discounting to incentivize efficient exploration and utilizes adaptive downsampling for stable distributed training. Extensive experiments demonstrate that IterResearch achieves substantial improvements over existing open-source agents with average +14.5pp across six benchmarks and narrows the gap with frontier proprietary systems. Remarkably, our
paradigm exhibits unprecedented interaction scaling, extending to 2048 interactions with dramatic performance gains (from 3.5% to 42.5%), and serves as an effective prompting strategy, improving frontier models by up to 19.2pp over ReAct
on long-horizon tasks. These findings position IterResearch as a versatile solution
for long-horizon reasoning, effective both as a trained agent and as a prompting
paradigm for frontier models.&lt;/p&gt;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="//localhost:1313/projects/iter-research/feature.png"/></item><item><title>Awe-Bench: Next-Generation Evaluation for AI Agents</title><link>//localhost:1313/projects/awe-bench/</link><pubDate>Tue, 20 Jan 2026 00:00:00 +0000</pubDate><guid>//localhost:1313/projects/awe-bench/</guid><description>Awe-Bench is our evaluation benchmark suite designed to rigorously test AI agents on real-world software engineering challenges. It goes beyond traditional benchmarks by incorporating diverse task types, realistic development environments, and fine-grained evaluation metrics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="//localhost:1313/projects/awe-bench/feature.svg"/></item><item><title>Multimodal Reasoning: Bridging Vision and Language</title><link>//localhost:1313/projects/multimodal-reasoning/</link><pubDate>Sat, 10 Jan 2026 00:00:00 +0000</pubDate><guid>//localhost:1313/projects/multimodal-reasoning/</guid><description>Our multimodal reasoning research focuses on building AI systems that seamlessly integrate visual perception with language understanding. We explore novel architectures for cross-modal reasoning, enabling agents to understand documents, diagrams, UIs, and real-world scenes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="//localhost:1313/projects/multimodal-reasoning/feature.svg"/></item><item><title>Code Generation: From Intent to Implementation</title><link>//localhost:1313/projects/code-generation/</link><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate><guid>//localhost:1313/projects/code-generation/</guid><description>Our code generation research explores cutting-edge techniques for translating natural language intent into production-quality code. We develop specialized models and training pipelines that understand programming semantics, project context, and software engineering best practices.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="//localhost:1313/projects/code-generation/feature.svg"/></item><item><title>Agent Framework: Building Blocks for Autonomous AI</title><link>//localhost:1313/projects/agent-framework/</link><pubDate>Fri, 28 Nov 2025 00:00:00 +0000</pubDate><guid>//localhost:1313/projects/agent-framework/</guid><description>Our Agent Framework provides the foundational building blocks for creating autonomous AI systems. It offers a modular architecture with pluggable components for reasoning, tool use, memory management, and environment interaction — enabling rapid prototyping of novel agent designs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="//localhost:1313/projects/agent-framework/feature.svg"/></item><item><title>SWE Data Engine: Scalable Training Data for Code Agents</title><link>//localhost:1313/projects/data-engine/</link><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate><guid>//localhost:1313/projects/data-engine/</guid><description>The SWE Data Engine is our automated pipeline for generating high-quality training data from real-world software repositories. It transforms raw repository history into structured training examples — including bug fixes, feature implementations, and code reviews — at scale.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="//localhost:1313/projects/data-engine/feature.svg"/></item></channel></rss>