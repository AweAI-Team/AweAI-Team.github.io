<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Benchmark on Awe-AI</title><link>//localhost:1313/tags/benchmark/</link><description>Recent content in Benchmark on Awe-AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© {year} Awe-AI Team. All rights reserved.</copyright><lastBuildDate>Tue, 20 Jan 2026 00:00:00 +0000</lastBuildDate><atom:link href="//localhost:1313/tags/benchmark/index.xml" rel="self" type="application/rss+xml"/><item><title>Awe-Bench: Next-Generation Evaluation for AI Agents</title><link>//localhost:1313/projects/awe-bench/</link><pubDate>Tue, 20 Jan 2026 00:00:00 +0000</pubDate><guid>//localhost:1313/projects/awe-bench/</guid><description>Awe-Bench is our evaluation benchmark suite designed to rigorously test AI agents on real-world software engineering challenges. It goes beyond traditional benchmarks by incorporating diverse task types, realistic development environments, and fine-grained evaluation metrics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="//localhost:1313/projects/awe-bench/feature.svg"/></item></channel></rss>