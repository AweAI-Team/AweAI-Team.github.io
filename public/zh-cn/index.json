


[{"content":" Alice Zhang # Role: Team Lead \u0026amp; Senior Research Scientist\nFocus Areas: AI Agent Architectures, Large Language Models, Reinforcement Learning\nBio # Alice leads the Awe-AI Team with a focus on designing next-generation AI agent architectures. She received her Ph.D. in Computer Science from Tsinghua University, where she specialized in reinforcement learning and language model pre-training. Her research has been published at top venues including NeurIPS, ICML, and ACL.\nSelected Publications # \u0026ldquo;Autonomous Software Engineering with Multi-Agent Systems\u0026rdquo; — NeurIPS 2025 \u0026ldquo;Scaling Laws for Code Generation Models\u0026rdquo; — ICML 2025 \u0026ldquo;Context-Aware Planning in LLM Agents\u0026rdquo; — ACL 2025 Contact # Email: alice.zhang@example.com Google Scholar | GitHub ","date":"1 January 2026","externalUrl":null,"permalink":"/team/alice-zhang/","section":"Our Team","summary":"Team Lead focusing on AI agent architectures and large language model research. Ph.D. from Tsinghua University.","title":"Alice Zhang","type":"team"},{"content":" Bob Li # Role: Senior Engineer — Infrastructure \u0026amp; Systems\nFocus Areas: Distributed Systems, Container Orchestration, Platform Engineering\nBio # Bob is the infrastructure backbone of Awe-AI. He designed and built the SWALM Portal service and the container-based execution environment that powers all our agent evaluations. With 8+ years of experience in distributed systems, he ensures our AI agents have reliable, scalable, and secure execution environments.\nKey Contributions # Designed the Portal architecture for in-container operations Built the Environment Manager for automated container lifecycle management Developed the evaluation proxy service for scalable benchmarking Optimized container startup times by 60% through layered caching Contact # Email: bob.li@example.com GitHub ","date":"1 January 2026","externalUrl":null,"permalink":"/team/bob-li/","section":"Our Team","summary":"Senior Engineer specializing in distributed systems and AI infrastructure. Architect of the SWALM Portal and container orchestration platform.","title":"Bob Li","type":"team"},{"content":" Carol Wang # Role: Research Scientist — Multimodal AI\nFocus Areas: Vision-Language Models, Document Understanding, Visual Reasoning\nBio # Carol is a research scientist specializing in multimodal AI systems. Her work bridges computer vision and natural language processing, enabling AI agents to understand and reason over visual information. She received her Ph.D. from Peking University and has published extensively at CVPR, ECCV, and AAAI.\nSelected Publications # \u0026ldquo;Cross-Modal Reasoning for Software UI Understanding\u0026rdquo; — CVPR 2025 \u0026ldquo;Document-Aware Code Generation from Screenshots\u0026rdquo; — ECCV 2024 \u0026ldquo;Visual Chain-of-Thought Prompting\u0026rdquo; — AAAI 2025 Contact # Email: carol.wang@example.com Google Scholar | GitHub ","date":"1 January 2026","externalUrl":null,"permalink":"/team/carol-wang/","section":"Our Team","summary":"Research Scientist working on multimodal reasoning and vision-language models. Published at CVPR, ECCV, and AAAI.","title":"Carol Wang","type":"team"},{"content":" David Chen # Role: Research Engineer — Code Intelligence\nFocus Areas: Program Synthesis, Code Understanding, Developer Tools\nBio # David bridges the gap between research and product by building practical AI-powered developer tools. He has extensive experience with code representation learning, static analysis, and IDE integration. Before joining Awe-AI, he contributed to several open-source developer tools.\nKey Contributions # Developed the CodeAct agent implementation for automated coding tasks Built code search and retrieval pipelines for repository-level understanding Created developer tool integrations for VS Code and JetBrains IDEs Contributed to the SWE-Bench evaluation framework Contact # Email: david.chen@example.com GitHub ","date":"1 January 2026","externalUrl":null,"permalink":"/team/david-chen/","section":"Our Team","summary":"Research Engineer focused on code intelligence, program synthesis, and AI-assisted software development tools.","title":"David Chen","type":"team"},{"content":" Emma Liu # Role: Research Scientist — Data \u0026amp; Training\nFocus Areas: Data Curation, Model Fine-Tuning, RLHF, Alignment\nBio # Emma leads our data and training pipeline efforts. She designs the data collection, filtering, and curation strategies that produce high-quality training data for our code-specialized models. Her expertise in RLHF and alignment techniques ensures our models are both capable and reliable.\nKey Contributions # Designed the SWE Data Engine pipeline for automated training data generation Developed quality filtering algorithms that improved training data utility by 40% Implemented RLHF training loops for code agent alignment Created preference data generation from agent trajectories Contact # Email: emma.liu@example.com Google Scholar | GitHub ","date":"1 January 2026","externalUrl":null,"permalink":"/team/emma-liu/","section":"Our Team","summary":"Research Scientist specializing in training data curation, model fine-tuning, and alignment techniques for code LLMs.","title":"Emma Liu","type":"team"},{"content":" Frank Wu # Role: Engineer — Evaluation \u0026amp; Benchmarking\nFocus Areas: Benchmark Design, Evaluation Infrastructure, Performance Analysis\nBio # Frank is responsible for building and maintaining our evaluation infrastructure. He designs benchmarks that meaningfully measure agent capabilities and builds the tooling needed to run evaluations at scale. His work ensures we have reliable, reproducible measurements of agent performance.\nKey Contributions # Built the Awe-Bench evaluation suite with 500+ real-world tasks Designed the Terminal-Bench benchmark for command-line agent evaluation Created automated regression testing for agent releases Developed performance dashboards and analytics pipelines Contact # Email: frank.wu@example.com GitHub ","date":"1 January 2026","externalUrl":null,"permalink":"/team/frank-wu/","section":"Our Team","summary":"Engineer specializing in evaluation systems, benchmark design, and performance analysis for AI agents.","title":"Frank Wu","type":"team"},{"content":"","date":"31 January 2026","externalUrl":null,"permalink":"/tags/agent/","section":"Tags","summary":"","title":"Agent","type":"tags"},{"content":"","date":"31 January 2026","externalUrl":null,"permalink":"/tags/agentic-rl/","section":"Tags","summary":"","title":"Agentic RL","type":"tags"},{"content":"","date":"31 January 2026","externalUrl":null,"permalink":"/categories/artificial-intelligence/","section":"Categories","summary":"","title":"Artificial Intelligence","type":"categories"},{"content":"","date":"31 January 2026","externalUrl":null,"permalink":"/tags/deep-research/","section":"Tags","summary":"","title":"Deep Research","type":"tags"},{"content":"","date":"31 January 2026","externalUrl":null,"permalink":"/authors/guoxin-chen-zile-qiao-xuanzhong-chen-donglei-yu-haotian-xu-wayne-xin-zhao-ruihua-song-wenbiao-yin-huifeng-yin-liwen-zhang-kuan-li-minpeng-liao-yong-jiang-pengjun-xie-fei-huang-jingren-zhou/","section":"Authors","summary":"","title":"Guoxin Chen, Zile Qiao, Xuanzhong Chen, Donglei Yu, Haotian Xu, Wayne Xin Zhao, Ruihua Song, Wenbiao Yin, Huifeng Yin, Liwen Zhang, Kuan Li, Minpeng Liao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou","type":"authors"},{"content":" Overview # Recent advances in deep-research agents have shown promise for autonomous knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates all information in a single, expanding context window, leading to context suffocation and noise contamination that limit their effectiveness on long-horizon tasks. We introduce IterResearch, a novel iterative deep-research paradigm that revisits long-horizon research through the lens of Interaction Scaling. Instead of relying on linear context accumulation, we adopt an MDP-inspired architecture with strategic workspace reconstruction. By maintaining an evolving report as memory and periodically synthesizing insights, our approach preserves consistent reasoning capacity across arbitrary exploration depths. To effectively train this paradigm, we employ Efficiency-Aware Policy Optimization (EAPO), a training strategy that adapts geometric reward discounting to incentivize efficient exploration and utilizes adaptive downsampling for stable distributed training. Extensive experiments demonstrate that IterResearch achieves substantial improvements over existing open-source agents with average +14.5pp across six benchmarks and narrows the gap with frontier proprietary systems. Remarkably, our paradigm exhibits unprecedented interaction scaling, extending to 2048 interactions with dramatic performance gains (from 3.5% to 42.5%), and serves as an effective prompting strategy, improving frontier models by up to 19.2pp over ReAct on long-horizon tasks. These findings position IterResearch as a versatile solution for long-horizon reasoning, effective both as a trained agent and as a prompting paradigm for frontier models.\nKey Features # We propose IterResearch, a novel iterative deep-research paradigm that revisits long-horizon research through an MDP-based formalism, maintaining sustained reasoning capacity through periodic synthesis and an evolving report memory—eliminating the context suffocation and noise contamination that plague mono-contextual approaches. We employ Efficiency-Aware Policy Optimization (EAPO) which adapts geometric discounted rewards that incentivize efficient exploration and utilizes adaptive downsampling for stable distributed training, enabling effective learning from our paradigm’s unique trajectory structure. Architecture # The system follows a modular monorepo structure:\nswalm_agent/ ├── packages/ │ ├── swalm-core/ # Core: Agent, Task, Tool, Client │ ├── swalm-portal/ # Portal: In-container operations │ ├── env-manager/ # Environment management │ └── eval-proxy/ # Evaluation proxy ├── configs/ # Hydra configuration └── runtime/ # Portal runtime environment Supported Agents # Agent Description Use Case CodeActAgent OpenHands CodeAct implementation General SWE tasks SWEAgent Princeton SWE-Agent Bug fixing \u0026amp; feature development ClineAgent Cline-style Agent Interactive development ReactAgent ReAct search agent Information retrieval SeedGeneralAgent General purpose agent Multi-domain tasks Results # Our agents have achieved competitive results across multiple benchmarks, demonstrating strong capability in autonomous software engineering.\nGetting Started # # Install dependencies uv pip install -e \u0026#34;packages/swalm-core[dev]\u0026#34; uv pip install -e \u0026#34;packages/swalm-portal\u0026#34; # Run a simple example uv run packages/swalm-core/src/swalm/agent/react/agent/example.py Links # GitHub Repository Documentation ","date":"31 January 2026","externalUrl":null,"permalink":"/projects/swalm-agent/","section":"Projects \u0026 Research","summary":"Overview # Recent advances in deep-research agents have shown promise for autonomous knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates all information in a single, expanding context window, leading to context suffocation and noise contamination that limit their effectiveness on long-horizon tasks. We introduce IterResearch, a novel iterative deep-research paradigm that revisits long-horizon research through the lens of Interaction Scaling. Instead of relying on linear context accumulation, we adopt an MDP-inspired architecture with strategic workspace reconstruction. By maintaining an evolving report as memory and periodically synthesizing insights, our approach preserves consistent reasoning capacity across arbitrary exploration depths. To effectively train this paradigm, we employ Efficiency-Aware Policy Optimization (EAPO), a training strategy that adapts geometric reward discounting to incentivize efficient exploration and utilizes adaptive downsampling for stable distributed training. Extensive experiments demonstrate that IterResearch achieves substantial improvements over existing open-source agents with average +14.5pp across six benchmarks and narrows the gap with frontier proprietary systems. Remarkably, our paradigm exhibits unprecedented interaction scaling, extending to 2048 interactions with dramatic performance gains (from 3.5% to 42.5%), and serves as an effective prompting strategy, improving frontier models by up to 19.2pp over ReAct on long-horizon tasks. These findings position IterResearch as a versatile solution for long-horizon reasoning, effective both as a trained agent and as a prompting paradigm for frontier models.\n","title":"IterResearch: Rethinking Long-Horizon Agents with Interaction Scaling","type":"projects"},{"content":"Discover the latest advancements from the Awe-AI Team. We are committed to open research and building tools that push the boundaries of what AI can achieve.\n","date":"31 January 2026","externalUrl":null,"permalink":"/projects/","section":"Projects \u0026 Research","summary":"Discover the latest advancements from the Awe-AI Team. We are committed to open research and building tools that push the boundaries of what AI can achieve.\n","title":"Projects \u0026 Research","type":"projects"},{"content":"","date":"20 January 2026","externalUrl":null,"permalink":"/authors/awe-ai-team/","section":"Authors","summary":"","title":"Awe-AI Team","type":"authors"},{"content":" Overview # Awe-Bench is our comprehensive evaluation benchmark suite that rigorously tests AI agents on real-world software engineering challenges. Unlike traditional benchmarks, Awe-Bench incorporates diverse task types, realistic containerized development environments, and fine-grained evaluation metrics.\nKey Features # Real-World Tasks: Curated from actual open-source repository issues and pull requests Diverse Task Types: Bug fixing, feature implementation, code refactoring, documentation, and testing Containerized Evaluation: Each task runs in an isolated Docker environment with pre-configured dependencies Multi-Dimensional Metrics: Beyond pass/fail — measures code quality, test coverage, and solution efficiency Scalable Infrastructure: Supports parallel evaluation across hundreds of tasks Benchmark Categories # SWE Tasks # Traditional software engineering tasks derived from popular open-source projects, including Django, Flask, scikit-learn, and more.\nTerminal Tasks # Command-line oriented challenges that test an agent\u0026rsquo;s ability to navigate file systems, manage processes, and use development tools effectively.\nMath Reasoning # Mathematical problem-solving tasks that require formal reasoning and proof construction capabilities.\nGetting Started # # Run a single SWE task evaluation uv run packages/swalm-core/src/swalm/task/swe/examples/run_single.py # Run the full benchmark suite python -m swalm.eval --config configs/eval/awe_bench.yaml Links # Benchmark Leaderboard Dataset on HuggingFace ","date":"20 January 2026","externalUrl":null,"permalink":"/projects/awe-bench/","section":"Projects \u0026 Research","summary":"Awe-Bench is our evaluation benchmark suite designed to rigorously test AI agents on real-world software engineering challenges. It goes beyond traditional benchmarks by incorporating diverse task types, realistic development environments, and fine-grained evaluation metrics.","title":"Awe-Bench: Next-Generation Evaluation for AI Agents","type":"projects"},{"content":"","date":"20 January 2026","externalUrl":null,"permalink":"/tags/benchmark/","section":"Tags","summary":"","title":"Benchmark","type":"tags"},{"content":"","date":"20 January 2026","externalUrl":null,"permalink":"/tags/evaluation/","section":"Tags","summary":"","title":"Evaluation","type":"tags"},{"content":"","date":"20 January 2026","externalUrl":null,"permalink":"/tags/open-source/","section":"Tags","summary":"","title":"Open-Source","type":"tags"},{"content":"","date":"20 January 2026","externalUrl":null,"permalink":"/categories/research/","section":"Categories","summary":"","title":"Research","type":"categories"},{"content":"","date":"10 January 2026","externalUrl":null,"permalink":"/tags/multimodal/","section":"Tags","summary":"","title":"Multimodal","type":"tags"},{"content":" Overview # Our multimodal reasoning research focuses on building AI systems that seamlessly integrate visual perception with language understanding. We explore novel architectures for cross-modal reasoning, enabling AI agents to understand documents, diagrams, user interfaces, and real-world scenes.\nResearch Directions # Document Understanding # Training models to parse and reason over complex documents including tables, charts, and mixed-format layouts.\nUI Comprehension # Enabling AI agents to understand graphical user interfaces, supporting automated testing and interaction.\nVisual Reasoning # Developing models that can perform step-by-step reasoning over visual inputs, combining perception with logical deduction.\nKey Results # State-of-the-art performance on document QA benchmarks Novel cross-attention mechanisms for vision-language alignment Efficient training strategies for multimodal foundation models Applications # Our multimodal reasoning capabilities power several downstream applications:\nAutomated Code Review: Understanding code changes alongside visual diffs Bug Report Triage: Parsing screenshots and error messages together Documentation Generation: Creating descriptions from UI screenshots Links # Research Paper (ArXiv) Model Weights (HuggingFace) ","date":"10 January 2026","externalUrl":null,"permalink":"/projects/multimodal-reasoning/","section":"Projects \u0026 Research","summary":"Our multimodal reasoning research focuses on building AI systems that seamlessly integrate visual perception with language understanding. We explore novel architectures for cross-modal reasoning, enabling agents to understand documents, diagrams, UIs, and real-world scenes.","title":"Multimodal Reasoning: Bridging Vision and Language","type":"projects"},{"content":"","date":"10 January 2026","externalUrl":null,"permalink":"/tags/research/","section":"Tags","summary":"","title":"Research","type":"tags"},{"content":"","date":"10 January 2026","externalUrl":null,"permalink":"/tags/vision-language/","section":"Tags","summary":"","title":"Vision-Language","type":"tags"},{"content":"","date":"1 January 2026","externalUrl":null,"permalink":"/tags/code-intelligence/","section":"Tags","summary":"","title":"Code Intelligence","type":"tags"},{"content":"","date":"1 January 2026","externalUrl":null,"permalink":"/tags/data/","section":"Tags","summary":"","title":"Data","type":"tags"},{"content":"","date":"1 January 2026","externalUrl":null,"permalink":"/tags/engineering/","section":"Tags","summary":"","title":"Engineering","type":"tags"},{"content":"","date":"1 January 2026","externalUrl":null,"permalink":"/tags/infrastructure/","section":"Tags","summary":"","title":"Infrastructure","type":"tags"},{"content":"","date":"1 January 2026","externalUrl":null,"permalink":"/tags/leadership/","section":"Tags","summary":"","title":"Leadership","type":"tags"},{"content":"We are a diverse group of researchers, engineers, and builders who share a passion for advancing artificial intelligence. Get to know the people behind our projects.\n","date":"1 January 2026","externalUrl":null,"permalink":"/team/","section":"Our Team","summary":"We are a diverse group of researchers, engineers, and builders who share a passion for advancing artificial intelligence. Get to know the people behind our projects.\n","title":"Our Team","type":"team"},{"content":"","date":"1 January 2026","externalUrl":null,"permalink":"/tags/training/","section":"Tags","summary":"","title":"Training","type":"tags"},{"content":"","date":"15 December 2025","externalUrl":null,"permalink":"/tags/code-generation/","section":"Tags","summary":"","title":"Code Generation","type":"tags"},{"content":" Overview # Our code generation research explores cutting-edge techniques for translating natural language intent into production-quality code. We develop specialized models and training pipelines that understand programming semantics, project context, and software engineering best practices.\nKey Innovations # Context-Aware Generation # Our models leverage repository-level context — including file dependencies, coding conventions, and API usage patterns — to generate code that fits naturally within existing codebases.\nMulti-Step Planning # Rather than generating code in a single pass, our approach decomposes complex requirements into a sequence of planned steps, improving both accuracy and maintainability.\nSelf-Repair Mechanisms # Built-in iterative refinement allows models to detect and fix errors in their own generated code through test execution and static analysis feedback loops.\nSupported Languages # Python, JavaScript/TypeScript, Java, C++, Go, Rust SQL, Bash/Shell scripting Infrastructure-as-Code (Terraform, Docker, Kubernetes) Benchmarks # Benchmark Score Rank HumanEval 92.1% Top 3 MBPP 88.5% Top 5 SWE-Bench Lite 45.2% Top 5 Links # Model on HuggingFace Technical Report ","date":"15 December 2025","externalUrl":null,"permalink":"/projects/code-generation/","section":"Projects \u0026 Research","summary":"Our code generation research explores cutting-edge techniques for translating natural language intent into production-quality code. We develop specialized models and training pipelines that understand programming semantics, project context, and software engineering best practices.","title":"Code Generation: From Intent to Implementation","type":"projects"},{"content":"","date":"15 December 2025","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":" Overview # Our Agent Framework provides the foundational building blocks for creating autonomous AI systems. It offers a modular architecture with pluggable components for reasoning, tool use, memory management, and environment interaction.\nArchitecture # The framework follows a clean, layered design:\n┌─────────────────────────────────┐ │ Agent Layer │ │ (CodeAct, SWE, Cline, etc.) │ ├─────────────────────────────────┤ │ Tool Layer │ │ (Shell, Editor, Browser, etc.) │ ├─────────────────────────────────┤ │ Client Layer │ │ (Portal, EnvManager, LLM) │ ├─────────────────────────────────┤ │ Infrastructure Layer │ │ (Container, Runtime, Logging) │ └─────────────────────────────────┘ Key Components # Agent Base Class # A flexible base class that handles the core agent loop: observe → think → act → evaluate.\nTool Registry # A dynamic tool registration system supporting both stateless tools (search, browsing) and stateful tools (file editing, shell execution).\nMemory Management # Configurable memory strategies including sliding window, summarization, and retrieval-augmented context management.\nEnvironment Abstraction # Unified interface for local execution, container-based sandboxing, and remote environment management.\nQuick Start # from swalm.core.agent.base import AgentBase class MyAgent(AgentBase): def __init__(self, llm_config, portal_config): super().__init__(llm_config, portal_config) self.tools = [...] async def run(self, prompt: str, max_turn: int = 30): # Your agent logic here pass Links # GitHub Repository API Documentation ","date":"28 November 2025","externalUrl":null,"permalink":"/projects/agent-framework/","section":"Projects \u0026 Research","summary":"Our Agent Framework provides the foundational building blocks for creating autonomous AI systems. It offers a modular architecture with pluggable components for reasoning, tool use, memory management, and environment interaction — enabling rapid prototyping of novel agent designs.","title":"Agent Framework: Building Blocks for Autonomous AI","type":"projects"},{"content":"","date":"28 November 2025","externalUrl":null,"permalink":"/tags/framework/","section":"Tags","summary":"","title":"Framework","type":"tags"},{"content":"","date":"28 November 2025","externalUrl":null,"permalink":"/categories/open-source/","section":"Categories","summary":"","title":"Open-Source","type":"categories"},{"content":" Overview # The SWE Data Engine is our automated pipeline for generating high-quality training data from real-world software repositories. It transforms raw repository history into structured training examples at scale.\nPipeline Stages # 1. Repository Mining # Automated collection and filtering of repositories based on quality metrics, activity levels, and language distribution.\n2. Issue-PR Linking # Intelligent matching of issues to their corresponding pull requests, creating natural (problem, solution) pairs.\n3. Environment Reproduction # Automated Docker environment generation that faithfully reproduces the build and test setup at each commit point.\n4. Quality Filtering # Multi-stage filtering pipeline that ensures data quality through:\nTest validation (solutions must pass existing tests) Diff complexity analysis Code quality scoring Deduplication 5. Data Formatting # Conversion into various training formats:\nSFT (Supervised Fine-Tuning) trajectories RLHF preference pairs Agent interaction traces Scale # Metric Count Repositories Processed 50,000+ Training Examples 500,000+ Languages Covered 15+ Docker Environments 10,000+ Links # Dataset on HuggingFace Pipeline Documentation ","date":"10 November 2025","externalUrl":null,"permalink":"/projects/data-engine/","section":"Projects \u0026 Research","summary":"The SWE Data Engine is our automated pipeline for generating high-quality training data from real-world software repositories. It transforms raw repository history into structured training examples — including bug fixes, feature implementations, and code reviews — at scale.","title":"SWE Data Engine: Scalable Training Data for Code Agents","type":"projects"},{"content":" Overview # The SWE Data Engine is our automated pipeline for generating high-quality training data from real-world software repositories. It transforms raw repository history into structured training examples at scale.\nPipeline Stages # 1. Repository Mining # Automated collection and filtering of repositories based on quality metrics, activity levels, and language distribution.\n2. Issue-PR Linking # Intelligent matching of issues to their corresponding pull requests, creating natural (problem, solution) pairs.\n3. Environment Reproduction # Automated Docker environment generation that faithfully reproduces the build and test setup at each commit point.\n4. Quality Filtering # Multi-stage filtering pipeline that ensures data quality through:\nTest validation (solutions must pass existing tests) Diff complexity analysis Code quality scoring Deduplication 5. Data Formatting # Conversion into various training formats:\nSFT (Supervised Fine-Tuning) trajectories RLHF preference pairs Agent interaction traces Scale # Metric Count Repositories Processed 50,000+ Training Examples 500,000+ Languages Covered 15+ Docker Environments 10,000+ Links # Dataset on HuggingFace Pipeline Documentation ","date":"10 November 2025","externalUrl":null,"permalink":"/projects/iter-research/","section":"Projects \u0026 Research","summary":"The SWE Data Engine is our automated pipeline for generating high-quality training data from real-world software repositories. It transforms raw repository history into structured training examples — including bug fixes, feature implementations, and code reviews — at scale.","title":"SWE Data Engine: Scalable Training Data for Code Agents","type":"projects"},{"content":"","externalUrl":null,"permalink":"/zh-cn/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/zh-cn/","section":"Awe-AI","summary":"","title":"Awe-AI","type":"page"},{"content":"","externalUrl":null,"permalink":"/zh-cn/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/zh-cn/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/zh-cn/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]